{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6be268f7-39ce-44d3-b0ad-89f260945e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Define a sparksession and connect it to the spark cluster\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://hadoop-master:7077\") \\\n",
    "        .appName(\"DE-1-4-SparkSession\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.executor.instances\", 7) \\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.cores.max\", 14) \\\n",
    "        .config(\"spark.cores.min\", 14) \\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcd1639f-d7dd-4518-919e-4906dbca9f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=====================================================>(146 + 1) / 147]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- content_len: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- normalizedBody: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- summary_len: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read the data and print the schema for understanding\n",
    "reddit_df = spark_session.read.json(\"hdfs://hadoop-master:9000/user/hadoop/input/corpus-webis-tldr-17.json\")\n",
    "reddit_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cd40589-e449-4659-93c0-1626c663aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths to positive and negative word lists\n",
    "path_pos = \"./opinion-lexicon-English/positive-words.txt\"\n",
    "path_neg = \"./opinion-lexicon-English/negative-words.txt\"\n",
    "\n",
    "# create sets (hash tables) to store the positive and negative words\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "# read the content of the word files\n",
    "with open(path_pos, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        positive_words.add(line.strip())\n",
    "with open(path_neg, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        negative_words.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30421c52-2113-454b-a584-a9d6da873d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast the word sets so all nodes can access the data\n",
    "broadcast_positive_words = spark_session.sparkContext.broadcast(positive_words)\n",
    "broadcast_negative_words = spark_session.sparkContext.broadcast(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25b72200-7629-43ec-b13f-f0849a1f8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# function to use to define the udf\n",
    "def pre_process(summary):\n",
    "    # convert to lowercase\n",
    "    line = summary.lower()\n",
    "    \n",
    "    # tokenize the summaries and remove dots and commas\n",
    "    line = re.sub(r\"[.,]\",'',line).split(\" \")\n",
    "    \n",
    "    # count the number positive and negative words\n",
    "    positive_count = sum([word in broadcast_positive_words.value for word in line])\n",
    "    negative_count = sum([word in broadcast_negative_words.value for word in line])\n",
    "\n",
    "    # if more positive words return positive sentiment (1)\n",
    "    if positive_count > negative_count:\n",
    "        return (1,positive_count,negative_count, 1)\n",
    "        \n",
    "    # if more negative words return negative sentiment (-1)\n",
    "    elif negative_count > positive_count:\n",
    "        return (-1,positive_count,negative_count, 1)\n",
    "        \n",
    "    # else neutral (0)\n",
    "    else:\n",
    "        return (0, positive_count, negative_count, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ad30beb-c3f1-4bd6-892c-d2afd45b701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
    "\n",
    "# define a structure for the new column\n",
    "schema = StructType([\n",
    "    StructField(\"sentiment\", IntegerType(), False),\n",
    "    StructField(\"positive_count\", IntegerType(), False),\n",
    "    StructField(\"negative_count\", IntegerType(), False),\n",
    "    StructField(\"number_of_tweets\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "755b7c2f-86d3-42fb-b071-b8958e82582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 226.8636417388916 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "import time as time \n",
    "# clear cache for more accurate timings\n",
    "spark_session.catalog.clearCache()\n",
    "start_time = time.time()\n",
    "\n",
    "# read the data into dataframe\n",
    "reddit_df = spark_session.read.json(\"hdfs://hadoop-master:9000/user/hadoop/input/corpus-webis-tldr-17.json\")\n",
    "\n",
    "# drop the columns that are not needed\n",
    "reddit_df = reddit_df.drop(\"author\", \"body\", \"normalizedBody\", \"id\", \"subreddit_id\", \"title\", \"content\", \"content_len\", \"summary_len\")\n",
    "\n",
    "# filter out comments where the subreddit is NULL\n",
    "sampled_reddit_df = reddit_df.filter(reddit_df[\"subreddit\"] != \"NULL\")\n",
    "\n",
    "# define udf\n",
    "conotation_udf = udf(pre_process, schema)\n",
    "\n",
    "# apply udf and create new column\n",
    "reddit_df_conotation = sampled_reddit_df.withColumn(\"conotation\", conotation_udf(\"summary\"))\n",
    "\n",
    "# map the subreddit of the comment to its sentiment and a counter keeping track of the number of total comments in subreddit\n",
    "grouped = reddit_df_conotation.rdd.map(lambda x: (x[\"subreddit\"], (x[\"conotation\"][\"sentiment\"], x[\"conotation\"][\"number_of_tweets\"])))\n",
    "\n",
    "# reduce by the key, sentiments add up to a total sentiment of the subreddit\n",
    "# the counter adds up to keep track of the total comments on a subreddit\n",
    "grouped = grouped.reduceByKey(lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "\n",
    "# take(1) to ensure not only lazy operations are made\n",
    "grouped.take(1)\n",
    "end_time = time.time()\n",
    "reddit_df.unpersist()\n",
    "print(f\"Total time: {end_time-start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39d3642b-cd93-4b0c-aeb9-6f9eae7e83af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('leagueoflegends', (5234, 109307)),\n",
       " ('buildapc', (3695, 14817)),\n",
       " ('summonerschool', (2532, 13806)),\n",
       " ('DotA2', (1657, 22405)),\n",
       " ('Guildwars2', (1398, 10948)),\n",
       " ('magicTCG', (1384, 10624)),\n",
       " ('DestinyTheGame', (1325, 19878)),\n",
       " ('seduction', (1316, 8784)),\n",
       " ('personalfinance', (1312, 14403)),\n",
       " ('photography', (1243, 5157))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort to view the most positive subreddits\n",
    "grouped = grouped.sortBy(lambda x: x[1], ascending = False)\n",
    "grouped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "618f1fcb-6f3a-4a70-a6f0-d12559ba71e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AskReddit', (-121714, 589947)),\n",
       " ('relationships', (-75276, 352049)),\n",
       " ('tifu', (-17853, 52219)),\n",
       " ('relationship_advice', (-7130, 50416)),\n",
       " ('funny', (-6758, 40171)),\n",
       " ('WTF', (-6555, 25781)),\n",
       " ('trees', (-6271, 47286)),\n",
       " ('AdviceAnimals', (-6257, 40783)),\n",
       " ('politics', (-6144, 36518)),\n",
       " ('offmychest', (-5298, 17175))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same for negative subreddits\n",
    "grouped = grouped.sortBy(lambda x: x[1], ascending = True)\n",
    "grouped.take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
